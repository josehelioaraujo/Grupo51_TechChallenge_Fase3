{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josehelioaraujo/Grupo51_TechChallenge_Fase3/blob/main/Grupo51_TechChallenge_Fase3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **PosTech Inteligência Artificial Para Desenvolvedores - Fiap**\n",
        "###  **Projeto Challenge Fase 3 - Grupo 51**\n",
        "\n",
        "### **Objetivo**:\n",
        "- Projeto de **Fine Tuning** de um Modelo Foundation LLM(**LLaMA 3 8B**), utilizando o dataset \"**The AmazonTitles-1.3MM**\".\n",
        "\n",
        "### **Descrição do Arquivo**:\n",
        "  - Arquivo Colab Principal do Projeto Tech Challenge Fase 3\n",
        "\n",
        "### **Grupo 51/Alunos:**\n",
        "  \n",
        "| Matrícula                       | Nome do Aluno  | Email                                                  |\n",
        "| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
        "| RM355027 | José Hélio Araujo Andrade  | helioandrade@hotmail.com|\n",
        "| RM356210 | Bernardo Guimarães Tinti   | betinti@hotmail.com |\n",
        "\n",
        "## Introdução\n",
        "A implementação da solução desse projeto, foi dividida em 3 arquivos Colab,   visando dividir as funcionalidades por etapas, tais como:\n",
        "   - Etapa de Prepação dos Dados\n",
        "   - Etapa de Geração de Perguntas e Respostas - Sem Fine Tuning\n",
        "   - Etapa de Geração de Perguntas e Respostas - Com Fine Tuning\n",
        "\n",
        "\n",
        "# Entregáveis\n",
        "\n",
        "\n",
        "- [Repositório no GitHub](https://github.com/josehelioaraujo/Grupo51_TechChallenge_Fase3)   \n",
        "- [Video Apresentação no Youtube](https://youtu.be/RtSPUqY9Q1I)  \n",
        "\n",
        "\n",
        "\n",
        "### Arquivos no Google Colab\n",
        "\n",
        "| Descrição do Arquivo Colab | Link de acesso   |\n",
        "| ------------------------------------------------------------ |   ------------------------------------------------------------ |\n",
        "| Colab Principal(esta página)   | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1PWEjYtJVAXYlr2zqjj3Ts8r_gW31m_cV#scrollTo=48mnS5Es6O_y) |\n",
        "| Preparação do Dados   | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1QBcfFYk8ij5GZZ8lHKjMhzXUnTgcXUQN#scrollTo=lFz7PZnJBCg) |\n",
        "| Geração de Peguntas e Respostas - Sem Fine Tuning   | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1dsZ8I_LtLQ2d6H5fmQMw9JZP8NrzZ7at#scrollTo=PFioDDNoSFUf) |\n",
        "| Treinamento do Modelo e Geração de Perguntas e Respostas    | [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1C4ti87nuhmzfHzjwc-iqYitp883bZRGA) |\n",
        "\n",
        "  \n",
        "\n",
        "### Datasets no Hungging Face\n",
        "\n",
        "| Datasets                               | Descrição                                                  | Link de Acesso                                                  |\n",
        "| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
        "| Dataset Original |Dataset original que será usado no Fine Tuning no treinamento do modelo  | [Acessar](https://huggingface.co/datasets/helioandrade/amazon-titles-jsonl) |\n",
        "| Datasets com 'Chunks'  | Datasets que foram particionados a partir do dataset original | [Acessar](https://huggingface.co/datasets/helioandrade/amazon-titles) |\n",
        "| Datasets com Fine Tuning  | Datasets com Fine Tuning e treinados com perguntas e respostas | [Acessar](https://huggingface.co/datasets/helioandrade/amazon-titles-questions-answering/tree/main) |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Ferramentas utilizadas**\n",
        "- [Conversor texto/audio ](https://crikk.com/text-to-speech/portuguese/)- usado na criação do audio da narração\n",
        "- OBS Studio - usado na gravação do video de apresentação\n",
        "- DaVinci Resolve - usado na montagem e edição do video de apresentação\n",
        "\n",
        "\n",
        "# O Problema\n",
        "\n",
        "No Tech Challenge desta fase, você precisa executar o fine-tuning de um foundation model (Llama, BERT, MISTRAL etc.), utilizando o dataset \"The AmazonTitles-1.3MM\". O modelo treinado deverá:\n",
        "\n",
        "- Receber perguntas com um contexto obtido por meio do arquivo json “trn.json” que está contido dentro do dataset.\n",
        "- A partir do prompt formado pela pergunta do usuário sobre o título do produto, o modelo deverá gerar uma resposta baseada na pergunta do usuário trazendo como resultado do aprendizado do fine-tuning os dados da sua descrição.\n",
        "\n",
        "## Fluxo de trabalho atualizado:\n",
        "\n",
        "1. **Escolha do Dataset**:\n",
        "   - **Descrição**: O The AmazonTitles-1.3MM consiste em consultas textuais reais de usuários e títulos associados de produtos relevantes encontrados na Amazon e suas descrições, medidos por ações implícitas ou explícitas dos usuários.\n",
        "\n",
        "2. **Preparação do Dataset**:\n",
        "   - Faça o download do dataset AmazonTitles-1.3MM e utilize o arquivo “trn.json”. Nele, você utilizará as colunas “title” e “content”, que contêm título e descrição respectivamente. Prepare os prompts para o fine-tuning garantindo que estejam organizados de maneira adequada para o treinamento do modelo escolhido. Limpe e pré-processe os dados conforme necessário para o modelo escolhido.\n",
        "\n",
        "3. **Chamada do Foundation Model**:\n",
        "   - Importe o foundation model que será utilizado e faça um teste apresentando o resultado atual do modelo antes do treinamento (para que se obtenha uma base de análise após o fine-tuning), e então será possível avaliar a diferença do resultado gerado.\n",
        "\n",
        "4. **Execução do Fine-Tuning**:\n",
        "   - Execute o fine-tuning do foundation model selecionado (por exemplo, BERT, GPT, Llama) utilizando o dataset preparado. Documente o processo de fine-tuning, incluindo os parâmetros utilizados e qualquer ajuste específico realizado no modelo.\n",
        "\n",
        "5. **Geração de Respostas**:\n",
        "   - Configure o modelo treinado para receber perguntas dos usuários. O modelo deverá gerar uma resposta baseada na pergunta do usuário e nos dados provenientes do fine-tuning, incluindo as fontes fornecidas.\n",
        "\n",
        "## O que esperamos para o entregável?\n",
        "\n",
        "- Documento detalhando o processo de seleção e preparação do dataset.\n",
        "- Descrição do processo de fine-tuning do modelo, com detalhes dos parâmetros e ajustes utilizados. Código-fonte do processo de fine-tuning.\n",
        "- Um vídeo demonstrando o modelo treinado gerando respostas a partir de perguntas do usuário e utilizando o contexto obtido por meio do treinamento com o fine-tuning."
      ],
      "metadata": {
        "id": "48mnS5Es6O_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X4n7eIAks1bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descrição da Implementação da Solução do Problema\n",
        "\n",
        "## Breve Introdução Teórica\n",
        "\n",
        "# 1. Etapa de Preparação de Dados\n",
        "## 1.1 Objetivo:\n",
        "Esta etapa tem o objetivo de preparar os dados, tais como, faz limpeza de dados(removendo registros em brancos, duplicados, etc), fazer a divisão do dataset original(tamanho muito grande),  em partições menores(chunks), que serão usados no processo de Fine Tuning e geração dos datasets de perguntas e respostas.\n",
        "\n",
        "## 1.1 Funcionalidades\n",
        "- **Download do Dataset**:\n",
        "  - Obter o dataset \"The AmazonTitles-1.3MM\" e localizar o arquivo \"trn.json\".\n",
        "  \n",
        "- **Análise do Dataset**:\n",
        "  - Examinar as colunas “title” e “content” para entender a estrutura e o conteúdo.\n",
        "\n",
        "- **Limpeza e Pré-processamento**:\n",
        "  - Remover dados duplicados e irrelevantes.\n",
        "  - Normalizar texto (remover pontuação, converter para minúsculas, etc.).\n",
        "  - Organizar os dados em prompts adequados para o treinamento do modelo.\n",
        "\n",
        "- **Divisão dos Dados**:\n",
        "  - Dividir o dataset em conjuntos de treino e validação para evitar overfitting.\n",
        "\n",
        "# 2. Etapa Geração Perguntas e Resposta - Sem Fine Tuning\n",
        "\n",
        "## 2.1Objetivo\n",
        "  - Geração de perguntas e respostas sem a execução do Fine Tuning do modelo.\n",
        "\n",
        "## 2.2 Funcionalidades\n",
        "\n",
        "- Carregamento de Modelo de Linguagem, Geração de Respostas e Streaming de Texto\n",
        "- Utiliza as bibliotecas Unsloth e Transformers  \n",
        "\n",
        "- Unsloth é uma biblioteca otimizada para fine-tuning de modelos de linguagem,\n",
        "especialmente focada em melhorar a eficiência e reduzir o consumo de recursos\n",
        "\n",
        "- Transformers é uma biblioteca popular da Hugging Face que fornece arquiteturas de estado da arte para NLP.\n",
        "\n",
        "- A combinação Unsloth + Transformers oferece um equilíbrio entre eficiência e facilidade de uso.\n",
        "- É particularmente vantajosa para quem precisa fazer fine-tuning de LLMs grandes com recursos computacionais limitados.\n",
        "- O código demonstra uma implementação básica de um sistema de pergunta e resposta sem usar o Fine Tuning, usando um modelo de linguagem pré-treinado.\n",
        "- Utiliza técnicas de otimização como quantização de 4 bits para melhorar a eficiência do modelo.\n",
        "   \n",
        "# 3. Etapa Geração Perguntas e Resposta - Com Fine Tuning\n",
        "\n",
        "## 3.1 Introdução\n",
        "- O código apresentado implementa um processo completo de fine-tuning de um modelo de linguagem grande (LLM), especificamente uma variante do LLaMA 3 com 8 bilhões de parâmetros. O objetivo principal é adaptar este modelo pré-treinado para uma tarefa específica, utilizando um conjunto de dados personalizado.\n",
        "\n",
        "- O processo envolve várias etapas-chave:\n",
        "\n",
        "  - Preparação do ambiente e carregamento do modelo pré-treinado.\n",
        "  - Aplicação de técnicas de otimização de memória e eficiência computacional, como quantização de 4 bits e Parameter Efficient Fine-Tuning (PEFT).\n",
        "\n",
        "  - Carregamento e formatação de um dataset personalizado para o treinamento.\n",
        "  - Execução do fine-tuning utilizando o SFTTrainer, com configurações específicas para melhorar a eficiência e o desempenho do treinamento.\n",
        "Salvamento do modelo ajustado e sua posterior utilização para gerar respostas a partir de prompts do usuário.\n",
        "\n",
        "## 3.2 Objetivo\n",
        "- O objetivo final é criar um modelo de linguagem personalizado capaz de gerar respostas relevantes e contextualmente apropriadas para uma tarefa específica, demonstrando como LLMs podem ser adaptados eficientemente para aplicações particulares.  \n",
        "\n",
        "## 3.3 Funcionalidades\n",
        "\n",
        "- Preparação do ambiente e instalação de pacotes necessários\n",
        "- Importação de bibliotecas e definição de parâmetros globais\n",
        "- Carregamento de um modelo de linguagem pré-treinado (LLaMA 3 8B)\n",
        "\n",
        "- Configuração do modelo para Parameter Efficient Fine-Tuning (PEFT)\n",
        "- Carregamento e formatação de um dataset personalizado\n",
        "-  Configuração e execução do treinamento usando SFTTrainer\n",
        "-  Salvamento do modelo treinado\n",
        "- Recarregamento do modelo treinado para inferência\n",
        "-  Geração de respostas baseadas em prompts do usuário\n",
        "\n",
        "- Carrega um modelo pré-treinado junto com seu tokenizador, utilizando a biblioteca FastLanguageModel,para configurar tanto o modelo quanto o tokenizer.\n",
        "- O modelo que está sendo carregado é uma variante compactada para otimização de memória e desempenho,\n",
        " sendo executado em 4 bits para economizar memória e acelerar o processamento.\n",
        "\n",
        "- Modelo escolhido foi o llama-3-8b-bnb-4bit.\n",
        "O modelo é baseado na arquitetura LLaMA (Large Language Model Meta AI), possivelmente com 8 bilhões de parâmetros (representado por \"3-8b\") e otimizado para ser carregado e executado em um formato de 4 bits,  o que significa que ele foi quantizado para reduzir o uso de memória, tornando-o mais leve.\n",
        "- O termo bnb refere-se ao uso da técnica BitsAndBytes, uma ferramenta para compressão e execução de modelos de forma eficiente, especialmente em dispositivos de menor capacidade.\n",
        "\n",
        "- Foi utilizada a técnica chamada Parameter Efficient Fine-Tuning (PEFT).\n",
        "- A biblioteca PEFT é projetada para melhorar a eficiência do ajuste fino de grandes modelos de linguagem, reduzindo o número de parâmetros treináveis, o que, por sua vez,\n",
        " torna o treinamento mais rápido e econômico em termos de memória.\n",
        "\n",
        "- Após a realização do Fine Tuning no modelo LLM selecionado, utilizando o dataset gerado com as 'Perguntas e Respostas',e testando a inferência do modelo usando modelo treinado, o bot exibe a 'resposta', conforme a 'pergunta' feita pelo usuário,confirmando desta maneira, que a resposta está aderente ao contexto do modelo treinado e tunado\n",
        "  \n",
        "\n",
        "## 3.4 Descrição das bibliotecas utilizadas\n",
        "\n",
        "-  **unsloth**: Biblioteca para otimização de modelos de linguagem, fornecendo funcionalidades para carregar e configurar modelos de forma eficiente.\n",
        "\n",
        "- **trl**: Biblioteca para Reinforcement Learning em transformers, utilizada aqui para o treinamento supervisionado (SFTTrainer).\n",
        "\n",
        "- **datasets**: Biblioteca para manipulação eficiente de conjuntos de dados, usada para carregar e processar o dataset de treinamento.\n",
        "\n",
        "-  **transformers**: Biblioteca principal para trabalhar com modelos de linguagem baseados em transformers, fornecendo classes como TrainingArguments e TextStreamer.\n",
        "\n",
        "- **torch**: Biblioteca de aprendizado de máquina, utilizada implicitamente para operações tensoriais e treinamento do modelo\n",
        "\n",
        "\n",
        "\n",
        "# 4. Conclusões\n",
        "\n",
        "- O código demonstra uma implementação básica de um sistema de pergunta e resposta usando um modelo de linguagem pré-treinado.\n",
        "\n",
        "- Utiliza técnicas de otimização como quantização de 4 bits para melhorar a eficiência do modelo.\n",
        "\n",
        "- A abordagem permite uma interação em tempo real com o modelo, gerando respostas de forma gradual.\n",
        "\n",
        "- A combinação Unsloth + Transformers oferece um equilíbrio entre eficiência e facilidade de uso.\n",
        "\n",
        "- É particularmente vantajosa para quem precisa fazer fine-tuning de LLMs grandes com recursos computacionais limitados.\n",
        "\n",
        "\n",
        "- O código implementa um pipeline completo de fine-tuning de um modelo de linguagem grande (LLM) usando técnicas de otimização de memória e eficiência computacional.\n",
        "- A abordagem utiliza PEFT para reduzir o número de parâmetros treináveis, tornando o processo mais eficiente em termos de recursos computacionais.\n",
        "\n",
        "- O modelo final é capaz de gerar respostas personalizadas com base no treinamento realizado, demonstrando a capacidade de adaptação a tarefas específicas.\n",
        "\n",
        "- Fizemos uploads dos  datasets necessários utilizados no projeto no Hugging a fim de facilitar e agilizar a realização do processo do Fine Tuning, evitando desta maneira, o uso de pastas armazemento dos dados em pastas locais do Google Drive.\n",
        "  \n",
        "# 4. Sugestões de Melhorias\n",
        "\n",
        "- Implementar validação cruzada ou utilizar um conjunto de dados de validação para monitorar o desempenho do modelo durante o treinamento e evitar overfitting.\n",
        "\n",
        "- Adicionar técnicas de regularização adicionais, como dropout nas camadas PEFT, para melhorar a generalização do modelo.\n",
        "\n",
        "- Experimentar com diferentes configurações de hiperparâmetros, como taxas de aprendizado, tamanhos de lote e número de épocas, para otimizar o desempenho do modelo.\n",
        "- Implementar mecanismos de avaliação automática da qualidade das respostas geradas, utilizando métricas como BLEU, ROUGE ou perplexidade.\n",
        "\n",
        "- **Tratamento de Erros**: Implementar tratamento de exceções para lidar com possíveis falhas durante o carregamento do modelo ou geração de respostas.\n",
        "\n",
        "- **Parâmetros Configuráveis**: Permitir que o usuário ajuste parâmetros como `max_new_tokens` e `max_seq_length` através de argumentos ou um arquivo de configuração.\n",
        "\n",
        "- **Múltiplos Modelos**: Implementar a capacidade de escolher entre diferentes modelos pré-treinados para comparação de desempenho.\n",
        "\n",
        "- **Interface de Usuário**: Desenvolver uma interface gráfica simples para facilitar a interação com o modelo.\n",
        "\n",
        "- **Logging**: Adicionar um sistema de logging para rastrear o uso e o desempenho do modelo ao longo do tempo.\n",
        "\n",
        "- **Otimização de Memória**: Implementar técnicas para liberar memória GPU após o uso, especialmente importante para execuções longas ou em ambientes com recursos limitados.\n",
        "\n",
        "- **Validação de Entrada**: Adicionar validação para a entrada do usuário, garantindo que ela não exceda o tamanho máximo suportado pelo modelo.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5I3ObxcYALWN"
      }
    }
  ]
}